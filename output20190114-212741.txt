twiml talk the podcast why interview interesting people doing interesting things in machine learning and artificial intelligence I'm your host Sam cherrington
 today we're joined by Solon barocas assistant professor of information science at Cornell University Solon is also the co-founder of the fairness accountability and transparency in machine learning Workshop that's hosted annually at conferences like icml Solon and I caught up to discuss his work on model interpretability and the legal and policy implications of the use of machine learning models
 in our conversation we discussed the gap between law policy and ML and how to build the bridge between them including formalizing ethical Frameworks for Michelle so look at his paper the intuitive appeal of explainable machines which proposes that explain ability is really two problems inscrutability and non intuitiveness and the disentangling the two allows us to better reason about the kind of explain ability that's really required in any given situation and now on to the show
 alright everyone I am on the line with Solon barocas Solon is an assistant professor of information science at Cornell University Solon welcome to this week in machine learning and AI thanks so much for having me why don't we get started by having you tell us a little bit about your background and how you got involved in machine learning and the way that I found my way to machine learning so I got you a couple years ago now and I actually graduated from what is essentially a media studies Department I worked with a particular Professor named Helen disembalm for many years has been one meeting people doing work on the Essex up information technology but I knew early on when I was a graduate student that I I was interested in what was more commonly called Data Mining then and decided to take some classes and got to know some of the faculty
 what kind of brought me up to speed in the fundamentals of machine learning and became very clear to me that there were raised by Machine learning and that those were quite different than the kinds of topics that people were disgusting quite speculative be without knowing very much about how much you love me actually worked in practice and so I went off to do with a citation that was really trying to translate some of these interesting questions I grow out of the technical aspects of machine learning into and it has met over a kind of a long. Of time becoming more and more familiar with machine learning and both the community of researchers and practitioners in this area to the extent now that I feel pretty comfortable and at home in the broader Machinery community in general and also taking the form of a trying to bring some of these normative concerns around privacy
 your fairness or transparency further into the machine learning community so making these issues first order questions for the field and that has included workshops for about five years now explicitly and transparency into the mainstream I've entered the community I think people if they know me at all through that work and it's been great actually because you know five years ago it was a pretty nice Anish group some great people from the start but still quite a small community and over the past two years in particular these have just become kind of mainstream issues for
 field such that papers are not being accepted at these conferences a bunch of best paper Awards have gone to papers on fairness recently and if you fall the news at all a lot of company is lo-tech company is it also be gone to take me seriously and begin to work on them in practice and I've had a couple years I really appreciate that that aspect of kind of bridging these two communities I find off and are found that a lot of people in machine learning care about these issues of Ethics but there aren't a lot of people that have both the machine learning background and a background in ethics and so you often observe you know what appears to be like Reinventing the wheel like coming up with ethical Frameworks from scratch when in fact you know there's clearly been a lot of work put into that kind of thing on the ethics side and it's just an issue of like
 connecting these these dots on tight connecting the two sides of the bread or something like question research and policy discussion is people from recognized early on that at least when it comes to questions of fairness and discrimination there were some existing ways of thinking about the issue which lent them self to formalization so just to give a kind of freaked out of here it's sort of a Rule song and discrimination law that if there is a disparity and outcome greater than 20% between let's say many women who are applying for a job that itself can be a trigger for a case to investigate whether in fact there is discrimination tall the 4th destroy it comes from the the regulator than Planet regulator and even though it's not enough to establish that some kind of decision making process
 stick to say like well we should try to minimize the disparity so that its never greater than 20% because this is an existing way of thinking about this to you the kind of work that machine cleaner people like to do which is kind of constrained optimization problem so you know how can I build classifiers are printed models that aim to achieve maximum performance subject this additional constraints and some of the earliest work on fairness and directly out of some of these principles from the law since then what happened though is that there's been a bunch of new ways of trying to formalize Notions of fairness that are really different and pretty much the attached from the way that the Law & policy Community think about them and you could maybe think about that as being in Reinventing the wheel or not being sufficiently sensitive to how much work has been gone already but part of what's been kind of
 interesting new ways of thinking about the problem that haven't come up in the Long Haul see discussion in part because perhaps people haven't had reason to think about them in and a way that these new formulations actually part of the issue so an example of this is you know there's a debate now whether difference differences not only in the accuracy of the model between different groups matter but whether differences in the Air Raids matter so you know it might matter the cross these rates but which one of those actually matters or matter at all that the field is going proposing these new ideas that don't have a need or obvious analog and law
 okay and also can potentially reveal that some of the ways that law policy I thought about these problems are incomplete or incoherent or we could actually do better so I think there's a lot of opportunity here on that what are you kind of mentioned you know broadly different formalisms and in ways to parse this issue are there some other examples that come to mind of the way folks of formalized ethical and fairness Frameworks applying them to machine learning so I think it a set of concerns have to do with just straight-up differences in the performance of models for different groups for men and women for people you could think about that using any number of different metrics why it's not just not just these kind of false positive
 Mission and Onondaga Farms to my question whether the date of the underlying training data itself is reliable it says even if the data is perfect there might be circumstances where we still observe disparities in the performance of his model is between groups and if that's the case you know what could we do about how do we actually try to avoid those situations that though actually starts with a different position it says we should never actually believed that the training to do we have is reliable and in fact there's good reason to believe that it's systemically unreliable and a particular way some example of this is you know you might think about how does the accuracy of a predictive policing model differ between groups that's the sort of question you get asked in the first camp and the second Camp which is concerned with the underlined quality they might say you actually don't have data that is a
 good representation of a true incidence of crime and Society so anything you might do around the edges concerning performance it's really not addressing the fundamental problem which is like you're learning from highly biased data in a slightly more serious version of this concern might be that maybe you are even using data that encode some kind of Prussia so it's not just the selection of sample of data is biased but that the training beta is it sell SunPass decision made by someone which was made in a prejudicial way thore the example of an employer trying to use machine learning to help it find good people to hire in the future and you might say let's take a look at which employees are retired in the past have gone on to be predicted successful and the Target Red Bull we might select in this model is the annual review score that we've given our past employees and so what you want them all to do
 who is to predict what the likely you no annual review score would be for any new job applicant given the training data from your your past employees the problem of course is that even though one of these was meant to be carefully considered and people are different research Empirical research the show this that score is going to be an inflected either potentially bio conscious Prejudice or implicit bias or even really realized that their assessment somehow suede pre-existing beliefs about gender or race or have you and so what ends up happening is the training data I can just in codes that prior Prejudice or bias and so the model is not learning to predict how people would have to be doing the job the models aren't you predict the future person
 give you a taste for some of these problems really require quite different responses right. There are situations where your model just won't perform as well and these other situations were the underlying data is actually unreliable require more potentially kind of more ambitious Solutions which try to just compensate for what you believe to be the underlying problem with the data even though you might have no direct way of measuring the thing you really care about is following the thread of the job performance reviews to what degree is that specific problem been explored and what kinds of approaches or Solutions have you seen folks taking with that
 what's a regression so I don't think there's any concrete case yet where we've been able to establish that the training data actually encodes the past discriminatory decisions of management but as a kind of thought experiment empirical work on how humans actually do I sign these kinds of scores to the employees does very good reason to believe that that training day to put in fact whatever Training Day people might put together would suffer from that kind of problem there's a lot of people who are more more familiar with this as a potential concern and so that's resulted both and lawyers trying to find cases of potentially bring against employees but separately it is also a lot of company specializing machine learning who are trying to integrate these concerns into the products the clients
 Craigslist to 10 developer tools that they they license or sell to two squares on the one hand and a lot of the simplest interventions involved report performance in a way that just breaks apart overall performance performance by group right so can we just observe that this model doesn't much worse job for certain people than others and are the ways that we can try to compensate for that and more common approach the problem with that approach of course is that you actually need to know those details about the people you're trying to score so in order to separate out the performance for men and women or for people with different races have to collect information about that and as a kind of standard practice employers that we really have good incentive not to collect information
 because I don't want to even create the possibility that a being accused of having considered at when making decisions so you might want an information but historically companies have been instructed not to collect it to limit the likelihood that they could ever even consider it the problem with and codeine constitutions which might have been influenced by human bias that's much more difficult and it's not even obvious with a principled approach to that would be some of the positions of people have put forward is to make certain assumptions about what you think is a more reasonable distribution of attributes across the population so that you should have kind of override what is the actual distribution in the trading day that you're doing with in a way you're basically saying I suspect the data to be systematically flawed and I'm going to put my thumb on the scale to compensate for that
 people now have pretty rigorous mathematical methods for doing this without necessarily sacrificing performance but I'm just not so familiar yet with what is happening to practice on that front and the final thing I would say is that you know any in a situation where you have other mechanisms to potentially measure these Dynamics you might take a different approach you might say I'm not going to treat this as up to your prediction problem I'm going to try to do some kind of empirical study to see if in fact you know there's a problem in my workplace so rather than just relying exclusively on his annual review scores maybe you go and try to find some other other thing to measure which is not as vulnerable to Human by us so an example of this might be something like well it's pretty hard to argue with the fact that you can achieve some sales figure at the end of the year of your sales person might like that seems like a a metric that's much less vulnerable to this kind of bias assessment
 you can challenge Zach but the argument anyway is it maybe we can begin to measure other things and then compare that to the kinds of assessment people get at the annual review and that might reveal some kind of underlying disparity in Barcelona misalignment between people's True Performance and the score their given any then given that finding you can go a long answer I haven't even exhausted the list of possibilities I mean I guess it's obvious that a lot of these challenges are like fundamental human organizational people issues and technology is but a small part of the the overall picture
 that's right and I think what's interesting about the current state of the research is that a lot of it is rough head to head comparison between existing and some model under a perfect condition and I think a lot of how to sync more, formerly and carefully about these concerns to perform the institution itself as you were saying right like when they consider how this fits into the bureaucratic decision-making process how it figures into the Dynamics of the workplace it would happen so even if we are successful in trying to be able to deal with these concerns within the model itself that is certainly not sufficient to achieve these brought her fairness or
 Justice Kohl's we might have about an example in the context of performance reviews but are there more broadly any examples that you've come across all of this process taking place full circle within an organization or some political structure where is Alvarez make per day tobias's were observe some sets of adjustments were made to modeling you know as well as the underlying organization organizational practices and that leading to a better outcome and I don't think there is some shining example to unfortunately I mean a certainly some examples of of a change of that been made that were little bit more straightforward so some Scholars were able to show that there were disparities and
 performance of the Shelf facial recognition software and engender such that job for instance for black women than for white men and the results of This research which I certainly encourage your listeners to to take a look at was some pretty rapid changes on the part of the companies that provided these often API facial recognition services so they just trying to figure out how to improve the performance for these populations you know that's a different story because it doesn't involve this entire can a workflow and bureaucracy which houses Pittsburgh the the county was working with some academic researchers to
 and old it's child protective welfare agency wants back it's also actually Focus the chapter of an excellent book called automated in a quality both describe the process by which researchers engage with the city but also with the agency workers with advocates for children and families for people affected by the system's Vision all the different interests that this agency was charged with serving tomorrow performed there it was likely to produce these kinds of kind of disparate impact in the way that it would suggest people for scrutiny when it came to potential child
 or child abuse and one of the interesting things to think about when focusing on the story is that despite the fact that this effort really involve a considerable amount of community engagement and consultation and even really into consideration some of these questions around people never lie still have that used to be store so it's hard to say that this is like a you know a clear model for what everyone should be doing and similar situations but it gives some sense of just how difficult it may be too kind of more fundamentally address questions of fairness even if you gone to the effort of integrating them into the model development process I think what are points do for me is the need for lots of people and perspectives to to get involved in understanding this issue and how it applies to the problems that they
 Rhode Island and you know taking on the little pieces of it that they can take on even if they're you know not exposed to a full kind of the full cycle if you will the way you think about this is is heavily influenced by the fact that one of the people who need to machine learning what someone who had more than a decade of experience and practice Foster Pro Bowl is a professor at NYU and his way of teaching in education is how do you translate a business problem into a reasonable and for people who is a familiar problem to know exactly how to solve some general prison problem
 I figured out how to specify the target variable and my son says that when people do think about that they think about it in terms of you know what is the appropriate thing to try to protect an online marketing write clearly like quick through is not the best thing we want someone to convert until you can have it pretty obvious debate around what is the right thing that you're you should be predicting I think it's a similar thing that happens in these domains that involve much more high-stakes decisions like you know child welfare or employment or credit right we can have I think I freeze pretty straightforward conversation about what it is that were actually trying to to achieving does the way the problem has been specified actually correspond to ardor me to call
 I have a pretty deep and thoughtful conversation with your stakeholders you want to understand the problem that you're being charged for solving and you want to understand whether or not it really really reflects the concerns of the people at supposed to serve so you know a lot of existing ways of doing machine learning well in practice those inside the kind of ideas people have from their experience in the ground could be super helpful to the conversation people are having now about ethics recently published paper that looked at the applicability of some of the work that's happening around transparency and explain ability to various regulatory Frameworks like gdpr others can you give us an overview of that work
 so this is a forthcoming paper and Fordham law review and my co-author and yourselves and I were trying to sort of dream together a couple different conversations that were happening in the lawn policy World there is a lot of anxiety around the use of machine learning for high stakes decision making because the fear is that you won't be able to explain the outcome of a decision making process and for people who come in you'll know that there is a long history of working on interpretability machine learning and that this is becoming a hot area as deep learning has become more successful more dominant and there's been trying to be able to explain what is that there's ways to swim to have these two things speak to each other a little bit part of it is about explaining exactly what it is existing laws and regulation
 actually require when they require explanations and then part of it is also trying to show that there are in many cases tools for satisfying those laws so although gdpr which is the European Jade General data protection regulation is the thing that has really generated a lot of attention around these issues recently there are here in the United States that also will require explanations for Autumn a decision and the key example about is the Equal Credit Opportunity Act and the Fair Credit Reporting Act these are both laws that regulate credit credit score and credit decision making and so in a way that when a person applies for credit and the Creditor denies that person the cutter actually have to give reasons for why they denied the loan
 and this law actually do the 1970s it's been around for a very very long time and it has practice you'll know that this really is the way that they've had to orient all the work I did make sure that they could always give reasons for the decision regardless of the mechanism by which they they got to their decision about whether to issue ballon the concern is that you know can you give reasons for a decision around credit if your model of using deep learning well if you know the work in machine learning at all you'll know that a lot of the recent proposals involved for going any attempt to actually provide Global transparency and forget any effort to describe the full relationship that the model Maps out and instead
 you some other mechanisms to see if we can say what in any given decision actually seem to be the most Salient variable or set of variables so which features in the model really account for this particular classification or or I'll come and those methods have proven pretty powerful and general for purposes of remodel but certainly it's not hard to imagine that they would be well-suited to existing requirement specific and the actual reasons why someone was denied alone so you can go off and build arbitrarily complex model so long as you can provide specific reasons for any particular decision might be a way to avoid
 the long-standing perceive trade-off between the performance of the model is built and then Shepherd ability of those models
 so it seems like a good outcome it seems like progress in research domain name has really helped solve a long-standing issue with the paper with my co-author kind of ghosts about why does might not always be so helpful and practice and it's a bit complicated while we definitely want to dig into what makes it complicated but I'm curious with what you you stated kind of the broader history of explain ability or transparency requirement you know about other regulations did you generally feel like the all of the hullabaloo about gdpr and its implications for a machine learning and deep-learning and Innovation and all the stuff like you feel it was overblown or you know based on some of the
 just that you're aware of appropriate DPR in general is appropriate me different from the existing National laws that the regulation is meant to replace in the European Union a regulation refers to a law that is standardized across all member states there was no protection directive for more than 15 years I think that was earlier version of what is now gdpr the difference is that the guy didn't remember States and they were expected to follow similar rules but it wasn't standardized across all of your the regulation was updated the kind of deal with some new problems but the change between existing laws and the regulation was so appropriate
 tell whether or not or or making sure that you obey the law for failing to comply with the law so my sense is that the motivation for taking this serious much more seriously anyway then it happened in the past as less that the law has changed dramatically is much more of that kind of consequences for significant you should have been this will not actually require a radical change the facts of matter is that most people were not even aware of the law let alone following it so that really account for difference
 that you explain the decision decision making in general is forbidden and less people give consent and even when they give consent you still have to be able to explain the decision and I guess I mention right there's a sense in which perhaps there is a trade-off between performing explain ability so even if this wouldn't in order to make sure that it remains explainable and degradation in performance that depending on how you could potentially satisfy this requirement without necessarily building a model that I sufficiently simple that even a lay person could be able to look at it and understand it
 the ways of providing exclamations which still satisfy the law that don't require this in a potential trade-off is this bring us back to the point that you are about to make about looking at the details of complying with these various loss in that direction so you know I think that the hope is that by explaining how decisions are made you will know whether or not that is a good way of making decisions on it so explanations or sort of a mechanism to check for other things you care about write check to see that the decision making process or in this case the model is taken into consideration had to check that it's not take into consideration things that it should be right so it shouldn't be considering explicitly things like race or it shouldn't be considering things that are arbitrary or clube irrelevant
 what are the challenges here is that sometimes you might be a situation where even if you explain how the bottle makes its decisions you a you as a human may not have any good intuitions for whether or not that is a reliable or sound basis for decision-making so you know one of the reasons people are are interested in machine learning is that it can uncover patterns and data said that would just ask a human's attention you don't know what would really be able to figure out that there's some kind of subtle signal across 10000 features that none of what you none of which on their face in particular relevance to the to the task at hand so if it turns out however I thought the model has found such a signal you could potentially try to explain the ones that are relevant to any particular decision but even if you did it would be impossible for humans and Noah like whether or not that's a reasonable or or or really appropriate right example example here
 if it turns out that the way you tie your shoes is predictive of song you know what kind of performance on the job on its face that seems sort of laughable right like why should that matter to my job performance but let's just say what basis do you actually say this is a good or bad model it's it's good in the sense that it's accurate but it's not choosing to pay attention to something irrelevant or obviously on Fair it's unsettling because it has discovered that there is something relevant in a picture that we as humans just cannot see if possibly relevant this is I think one of the real challenges here right so even if we use some of these awesome machine learning techniques to give explanations of models they may not help us humans to be able to assess whether those were even reasonable thing
 just rely on to make important decisions some of the work that is going into as opposed to trying to make your models you know your fundamental fairness technique you know being one of trying to make your models of blind to factors like race or gender actually taking part of the fairness challenge to build into your models these things like race and gender is part of the decision making and using whether the models can predict race for example in the features that you that you kind of build into your model as an indicator that your model maybe surreptitiously kind of making decisions based on race right it's there in the signal it's just not obvious to you yeah and so this is exactly the approach that some people have taken it's it's
 will my mom Liz is non-discriminatory because it doesn't consider race or gender explicitly but it turns out that other features are highly correlated so maybe what you want to do is just about those pictures that are highly correlated of impossible exercise at some point because we're sufficiently rich datasets it's almost certain that these kinds of details about you will be reflected in the other features that you have until it's not too obvious at what point you are supposed to stop right like how many things you would remove from your model until you feel comfortable and it's interesting because it actually relates to the other issue explained ability that we were just discussing the simple fact that some future is correlated with race or gender on its own is not enough to say that it's legitimate or illegal to actually consider that feature right and the reason for that is that there just happens to be an equality in society some people possess feature
 at a certain value at a higher or lower rate than others that's a fact potentially of the world and this kind of difference in the value of this picture means that we should be actually considering that feature is not itself sufficient argument even even when it comes to look like cases around discrimination and this is a complicated issue right cuz it may well be the reason that some future actually is correlated with brace for instance is because of a long history of racial discrimination so Princeton zip codes right they can be very informative when trying to predict the value of someone's home but of course at the same time because of long term housing so this is a tricky problem in other situations what happens is that people will my understanding from talking to people and practices that they will want to find out which feature
 Spartan correlated with razor gender in the model and rather than just rip them out because they happen to be highly correlated they actual just go look at them they'll say you know they'll have a kind of rank order list for the top is though those features are most most correlated with the sensitive feature race or gender were talking and then they look at it and they say is it okay is it like reasonable to consider this feature even though it is highly correlated with race and gender and in some cases like the ZIP code example you might say no and it was obvious historical reasons why this is not acceptable in some cases you might say yes you might feel like well this doesn't seem to be the result of some kind of past Injustice it may just reflect some true difference in the distribution of this feature in the population so we're going to stand by its relevance for the decision at hand what's an example that falls into that category
 unsurprisingly a science project disproportionately white better chance of succeeding on the job right that we could about like why it is that the population in the school's looks the way it does and we could potentially get to a point where I feel like it's reasonable to say don't consider the university someone wants you but I think I would make a reasonable and plausible case for the relevance of your University you graduated song you would say no no no
 wolf Rasta considerate
 and so it with all that like where where does that leave us in terms of its there's certainly a lot of kind of potential thick gray lines here in terms of the Counterpoint to I think that example was an example where there is some historical evidence of historical discrimination and certainly that's the case in this University example that you gave so it's not quite the opposite concrete examples of how folks have parsed through all of this with some kind of framework or is it kind of everyone making a judgment call based on what they think is right it's a great questions a really good way of putting I think the current state of affairs so I think for some people there feels like there should be some kind of bright-line the the kind of University using you know what your alma mater as a way to determine whether to hire you should be obviously
 reasonable Ram Fender other people who I think take the view that you know there's lots of reasons to be suspicious of the admissions policies of the quality of the high school education that people receive to prepare them to apply to college and you can go back even further right at the disadvantage you face as a person earlier in life that going to set you on this particular course so you know this may be frustrating but I think ultimately there are going to be his ongoing debate around how to even course this issue right for some people it will be clear that there are certain factors that despite how poorly they are they are legitimate to consider when building is modeled about the need to actually use the model development process to compensate for the unfair disadvantage that people had suffered earlier in their lives and and ultimately
 what does out of machine learning debate right this is not something that is peculiar to building machinery model this ultimately it's just the kind of long standing in general about the decision making in certain sightings about what is the appropriate role of discrimination law in general so it's unsurprising ultimately that some of these things are not settled or not going to be settled in part because people have been arguing about this for at least fifty years when it comes to discrimination law and for Millennia when it comes to questions around fairness so do we cover all of the points that you wanted to cover with regards to your paper
 say about it is exclamation to actually know whether or not some things are reasonable thing to consider you need to be able to explain what the model is doing humans actually look at it and see if I can leave some story that makes it feel like a reasonable basis for making these important decisions and some cases maybe what we should do instead is just abandon these requirements for explanations and focus on providing proof will not have certain problems and we can do that on exclamation and this just would have summarized as I think the point I was making a moment
 which is that this I think just ultimately depends on your your perspective for some people it will feel inadequate to just provide guarantees I can imagine myself actually satisfied with something I said like this system is certified fair but we're never going to tell you how it makes its decision at the same time kind of formula then what people expect they will get out of explanations and so I just think there's a roll here for both things in an opportunity to spend a lot more time figuring out when each of those is most appropriate I will summon thank you so much for taking the time to chat with me about this office is super interesting it and super important as well
 alright everyone that's our show for today for more information on Solon or any of the topics covered in the show visit puma.com / 219 as always thanks so much for listening and catch you next time
